{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Titanic : Machine Learning from Disaster From Scratch ! (Part 1 : Logistic Regression)\n",
    "\n",
    "***\n",
    "\n",
    "In Part 1 of this two part notebook , we will explore the idea of logistic regression as a single cell neural network and write a code to classify  the Kaggle Titanic: Machine Learning from Disaster dataset.\n",
    "\n",
    "## Step 1: Import Libraries and Data\n",
    "***\n",
    "Since we are trying to write our own logistic regression function , we will not be using any of the popular machine learning library such as scikit learn. We will only be using Numpy for matrix computations and Pandas to handle our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is in csv format. \n",
    "We use pandas to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "(418, 11)\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('Data/train.csv')\n",
    "data_test = pd.read_csv('Data/test.csv')\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize our dataset by taking a look at first five entries of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of various Attributes such as PassengerID , Name , Sex , Age etc and the attribute called \"survived\".\n",
    "Since we are trying to predict whether or not a passanger survived , we will store these values as labels of our dataset Y and rest of the attributes can be regarded as features.\n",
    "But, first we have to clean up for data and the first step is to look for missing data and fill it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are missing a lot of data about the age of passangers from both our training and testing datasets.\n",
    "We are also missing cabin numbers and the embarkation data, but we can fix that.\n",
    "\n",
    "\n",
    "### There are a couple of methods of dealing with missing data:\n",
    "- You can simply ignore the data which has missing entries, but if a significant amount of your data is missing then we will endup throwing away a lot of data.\n",
    "\n",
    "- Replace the missing data by the some value such as mean, median or mode of the rest of the data.\n",
    "\n",
    "Since we have a training set of 891 examples and a test set of 418 examples and we are missing a lot of age data, we would be throwing away a lot of the rest of the data if we simply remove the data entires with missing age values. Hence , we will try to fill missing value.\n",
    "As to what it should be filled with , we can fill them with mean , mode or median of the.\n",
    "There are other methods such as KNN , MICE or imputation using deeplearning but they are currently beyond our scope.\n",
    "\n",
    "For this experiment we can just try them all and see what works best !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that replaces missing data based on the method and key parameters\n",
    "def fill_nan(data, key, method = \"mean\"):\n",
    "    if method == \"mean\":\n",
    "        data[key].fillna(data[\"Age\"].mean(), inplace = True)\n",
    "    if method == \"mode\":\n",
    "        data[key].fillna(data[\"Age\"].mode()[0], inplace = True)\n",
    "    if method == \"median\":\n",
    "        data[key].fillna(data[\"Age\"].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our fill_nan() function will fill missing age values from our dataset depending on the method that we pass as a parameter.\n",
    "\n",
    "#### Lets try to fix missing embarkation and fare data.\n",
    "- Since Embarked is a string which defines what port a person boarded the titanic , the we fill the missing embarkation data with the mode of the dataset.\n",
    "\n",
    "- Fare data is a float value and again we can just repurpose our fill_nan() function to try out what works best in our case but since only one entry in the test dataset is missing this value , it wont make much of a difference. So , lets just fill it with the mean fare data\n",
    "\n",
    "But , before we make any changes to our data , lets copy our data into seperate variables so that we still have our original data incase we mess up !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make copys of our data. deep = true means that each entry of our data also gets copied to a different memory address\n",
    "data_train_cleaned = data_train.copy(deep = True)\n",
    "data_test_cleaned = data_test.copy(deep = True)\n",
    "\n",
    "#calculate stats of our data\n",
    "data_train_cleaned.describe(include = 'all')\n",
    "data_test_cleaned.describe(include = 'all')\n",
    "\n",
    "#clean data\n",
    "#fill empty age\n",
    "fill_nan(data_train_cleaned, \"Age\", \"median\")\n",
    "fill_nan(data_test_cleaned, \"Age\", \"median\")\n",
    "\n",
    "#fill empty embarked in train\n",
    "data_train_cleaned[\"Embarked\"].fillna(data_train_cleaned[\"Embarked\"].mode()[0], inplace = True)\n",
    "\n",
    "#fill empty fare in test\n",
    "data_test_cleaned[\"Fare\"].fillna(data_test_cleaned[\"Fare\"].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of \"Cabin\" is a string which tells the cabin number of the passanger . But since there is really no way to fill missing cabin numbers , we will simply drop that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_cleaned = data_train_cleaned.drop(\"Cabin\", axis = 1)\n",
    "data_test_cleaned = data_test_cleaned.drop(\"Cabin\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passanger ID , Name of teh passanger and Ticket number donot play a role in survival of a person , so these columns can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_cleaned = data_train_cleaned.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis = 1)\n",
    "data_test_cleaned = data_test_cleaned.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize our data again !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
      "0         0       3    male  22.0      1      0   7.2500        S\n",
      "1         1       1  female  38.0      1      0  71.2833        C\n",
      "2         1       3  female  26.0      0      0   7.9250        S\n",
      "3         1       1  female  35.0      1      0  53.1000        S\n",
      "4         0       3    male  35.0      0      0   8.0500        S\n",
      "Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data_train_cleaned.head())\n",
    "print(data_train_cleaned.isnull().sum())\n",
    "print(data_test_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are no Null values in our dataset and we have removed irrelevant data.\n",
    "Now , we need to swap the string values of Sex and Embarked by integers inorder to use them as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
      "0         0       3    1  22.0      1      0   7.2500         0\n",
      "1         1       1    0  38.0      1      0  71.2833         1\n",
      "2         1       3    0  26.0      0      0   7.9250         0\n",
      "3         1       1    0  35.0      1      0  53.1000         0\n",
      "4         0       3    1  35.0      0      0   8.0500         0\n",
      "   Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
      "0       3    1  34.5      0      0   7.8292         2\n",
      "1       3    0  47.0      1      0   7.0000         0\n",
      "2       2    1  62.0      0      0   9.6875         2\n",
      "3       3    1  27.0      0      0   8.6625         0\n",
      "4       3    0  22.0      1      1  12.2875         0\n"
     ]
    }
   ],
   "source": [
    "#map Sex of a passenger to interger values , female : 0 , male : 1\n",
    "data_train_cleaned['Sex'] = data_train_cleaned['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "data_test_cleaned['Sex'] = data_test_cleaned['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "#map embarked of a passenger to integer values S: 0, C : 1, Q : 2\n",
    "data_train_cleaned['Embarked'] = data_train_cleaned['Embarked'].map({'S' : 0, 'C' : 1, 'Q': 2}).astype(int)\n",
    "data_test_cleaned['Embarked'] = data_test_cleaned['Embarked'].map({'S' : 0, 'C' : 1, 'Q': 2}).astype(int)\n",
    "\n",
    "#visualize data\n",
    "print(data_train_cleaned.head())\n",
    "print(data_test_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned and prepare our data , we can proceed with programming our logistic regression unit.\n",
    "\n",
    "Let us first rearrange our data into X_train ( training dataset containng 70% of the data_train_cleaned) , Y_train ( labels of training dataset ), X_val ( our validation dataset which will be 30% of data_train_cleaned ), Y_val ( labels of our validation dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train : [[ 3.      1.     22.     ...  0.      7.25    0.    ]\n",
      " [ 1.      0.     38.     ...  0.     71.2833  1.    ]\n",
      " [ 3.      0.     26.     ...  0.      7.925   0.    ]\n",
      " ...\n",
      " [ 3.      1.     27.     ...  0.     14.4542  1.    ]\n",
      " [ 1.      1.     42.     ...  0.     52.5542  0.    ]\n",
      " [ 3.      1.     20.     ...  1.     15.7417  1.    ]]\n",
      "x val : [[ 3.      1.     21.     ...  0.      7.8542  0.    ]\n",
      " [ 3.      1.     21.     ...  0.     16.1     0.    ]\n",
      " [ 1.      1.     61.     ...  0.     32.3208  0.    ]\n",
      " ...\n",
      " [ 3.      0.     28.     ...  2.     23.45    0.    ]\n",
      " [ 1.      1.     26.     ...  0.     30.      1.    ]\n",
      " [ 3.      1.     32.     ...  0.      7.75    2.    ]]\n",
      "y train:[0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1.]\n",
      "y val : [0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1.\n",
      " 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0.]\n",
      "x train :(623, 7)\n",
      "x val :(268, 7)\n",
      "y train :(623,)\n",
      "y val :(268,)\n"
     ]
    }
   ],
   "source": [
    "#make a copy of our data to slice it\n",
    "X_data = data_train_cleaned.copy(deep = True).values # .values converst pandas dataframe to a numpy array\n",
    "\n",
    "#split data into train and val\n",
    "X_train = X_data[:623] #70% of our training data 891 is ~ 623 values\n",
    "X_val = X_data[623:] #30% of our training data is ~ 268 values\n",
    "\n",
    "# labels are \" survived \" column of the dataset\n",
    "Y_train = X_train[:,0]\n",
    "Y_val = X_val[:,0]\n",
    "\n",
    "#remove labels from dataset and only keep features\n",
    "X_train = np.delete(X_train, 0, axis = 1)\n",
    "X_val = np.delete(X_val, 0, axis = 1)\n",
    "\n",
    "#print data for sanity check\n",
    "print(\"x train : \" + str(X_train))\n",
    "print(\"x val : \" + str(X_val))\n",
    "print(\"y train:\"+ str(Y_train))\n",
    "print(\"y val : \" +str(Y_val))\n",
    "\n",
    "#print shapes for sanity check\n",
    "print(\"x train :\" + str(X_train.shape))\n",
    "print(\"x val :\" + str(X_val.shape))\n",
    "print(\"y train :\" + str(Y_train.shape))\n",
    "print(\"y val :\" + str(Y_val.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Labels are weird rank 1 arrays and our training sets are arranged such that eash row is a training feature and each column is an example.\n",
    "\n",
    "Let is rearrange it such that each column of our training set is a different training example and our labels are a one dimentional column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train :(7, 623)\n",
      "x val :(7, 268)\n",
      "y train :(623, 1)\n",
      "y val :(268, 1)\n"
     ]
    }
   ],
   "source": [
    "# rearrange data such that each column is a different training example\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "\n",
    "#fix our lable matrix\n",
    "Y_train = Y_train.reshape((623, 1))\n",
    "Y_val = Y_val.reshape((268, 1))\n",
    "\n",
    "#sanity check\n",
    "print(\"x train :\" + str(X_train.shape))\n",
    "print(\"x val :\" + str(X_val.shape))\n",
    "print(\"y train :\" + str(Y_train.shape))\n",
    "print(\"y val :\" + str(Y_val.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to write our logistic regression !\n",
    "\n",
    "\n",
    "As discussed earlier , Logistic regression can be thought of as a single cell neural network with $ W^{(n)} $ parameters, where $n$ is the number of features and a single bias $b$\n",
    "\n",
    "We use sigmoid activation function where $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$\n",
    "\n",
    "Lets take a look at the sigmoid activation function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x296ffbce548>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import matplotlib to plot our sigmoid function\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-10, 10, 200)\n",
    "\n",
    "#define sigmoid function\n",
    "def sigmoid(x):\n",
    "    sig = 1/(1 + np.exp(-x))\n",
    "    return sig\n",
    "\n",
    "plt.plot(x, sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the sigmoid function maps values on x axis to values on y axis between 0 and 1.\n",
    "This can be interpreted as the probability of an event occuring.\n",
    "In our case , probability being close to 1 means that the person may have survived the titanic disaster !\n",
    "\n",
    "A more detailed explanation as to why we use this function, can be found here : https://qr.ae/pNnoDa\n",
    "\n",
    "Other advantages of logistic regression is that for lower values i.e between -2.5 and 2.5 , the gradient of the function is quite large, leading to faster learning through gradient descent !\n",
    "\n",
    "Since small values mean that our algorithm learns faster , we standardize our dataset which is a form of feature scaling!\n",
    "You can learn more about feature scaling here : https://en.wikipedia.org/wiki/Feature_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 623)\n",
      "[[ 0.82770401 -1.57071272  0.82770401 ...  0.82770401 -1.57071272\n",
      "   0.82770401]\n",
      " [ 0.76764947 -1.30267789 -1.30267789 ...  0.76764947  0.76764947\n",
      "   0.76764947]\n",
      " [-0.5792537   0.67215525 -0.26640146 ... -0.1881884   0.98500749\n",
      "  -0.73567982]\n",
      " [ 0.42388749  0.42388749 -0.49947581 ...  0.42388749  0.42388749\n",
      "   0.42388749]\n",
      " [-0.48340844 -0.48340844 -0.48340844 ... -0.48340844 -0.48340844\n",
      "   0.78198425]]\n"
     ]
    }
   ],
   "source": [
    "# inorder to standardize our dataset , we will subtract all features of our dataset by their mean\n",
    "# and divide it by the standard deviation of the features.\n",
    "# this will center the data and normalize our values so that gradient descent converges faster !\n",
    "\n",
    "def standardize(data):\n",
    "    std_data = (data - data.mean(axis = 1, keepdims = True)) / data.std(axis = 1, keepdims = True)\n",
    "    return std_data\n",
    "\n",
    "X_train = standardize(X_train)\n",
    "X_val = standardize(X_val)\n",
    "\n",
    "#sanity check !\n",
    "print(X_train.shape)\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have Normalized and centered our data , We are ready to write the Forward , backward propagation and gradient descent !\n",
    "\n",
    "First, we write a function to initialize our parameters based on the number of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "def initialize_parameters(dim):\n",
    "    W = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have initialized our parameters , we can make a prediction $A$ such that $A = \\sigma(Z)$ where, $Z = W^T.X + b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "def forward_prop(X, W, b):\n",
    "#     #sanity check\n",
    "#     print(\"forward prop\")\n",
    "#     print(\"X shape:\" + str(X.shape))\n",
    "#     print(\"W shape:\" + str(W.shape))\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    return A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pass the value $Z$ to the sigmoid function that we wrote earlier to make a prediction.\n",
    "We need to use gradient descent algorithm to minimize the error of our prediction.\n",
    "\n",
    "We could use mean squared error , but as it turns out , for logistic regression , mean squared error leads\n",
    "to a non convex surface. Our gradient descent works best in cases where our error forms a convex surface.\n",
    "\n",
    "$$ \\operatorname{MSE}=\\frac{1}{m}\\sum_{i=1}^n(Y_i-\\hat{Y_i})^2 $$\n",
    "\n",
    "Where $Y$ is our actual lable , $\\hat{Y}$ is our prediction and $m$ are the number of training examples\n",
    "\n",
    "For logistic regression, we typically use something called as cross entropy loss, which leads to a nice convex surface for our gradient descent \n",
    "\n",
    "$$ \\operatorname{J} = -\\frac1m\\sum_{m=1}^M\\ \\bigg[y_n  \\log \\hat y_n + (1 - y_n)  \\log (1 - \\hat y_n)\\bigg] $$\n",
    "\n",
    "Learn More : https://rohanvarma.me/Loss-Functions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function\n",
    "def compute_cost(Y, A):\n",
    "#     #sanity check\n",
    "#     print(\"computing cost\")\n",
    "#     print(\"Y shape:\" + str(Y.shape))\n",
    "#     print(\"A shape:\" + str(A.shape))\n",
    "    J = - np.sum(np.dot(Y.T, np.log(A)) + np.dot((1 - Y).T, np.log(1 - A)))/Y.shape[0]\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed the cost / error of our predictions , we can compute the gradients which will be required for our gradient descent.\n",
    "\n",
    "We use the chain rule of multivariate calculus to calculate the gradient of our parameters W and B using chain rule.\n",
    "\n",
    "The relationship between our parameters $W$ and $b$ and our cost function $J$ can be defined as follows:\n",
    "\n",
    "$$ \\operatorname{Z} = W^TX + b $$\n",
    "$$ \\operatorname{A} = \\sigma(Z) $$\n",
    "$$ \\operatorname{J} = -\\frac1m\\sum_{m=1}^M\\ \\bigg[Y  \\log A + (1 - Y)  \\log (1 - A)\\bigg] $$\n",
    "\n",
    "By applying chain rule, \n",
    "\n",
    "$$ \\frac {\\partial J}{\\partial W} = \\frac {\\partial J}{\\partial A} \\frac {\\partial A}{\\partial Z} \\frac {\\partial Z}{\\partial W} $$\n",
    "\n",
    "And, \n",
    "\n",
    "$$ \\frac {\\partial J}{\\partial b} = \\frac {\\partial J}{\\partial A} \\frac {\\partial A}{\\partial Z} \\frac {\\partial Z}{\\partial b} $$\n",
    "\n",
    "If we work through the differentiation, we get the solutions :\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
    "\n",
    "Proof of this solution can be found here : https://stats.stackexchange.com/a/278812\n",
    "\n",
    "in our code, we will be writing the value $ \\frac{\\partial J}{\\partial w} $ as dW and value $\\frac{\\partial J}{\\partial b}$ as db.\n",
    "\n",
    "Let use proceed to writing our back propagtion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back prop function\n",
    "def back_prop(X, Y, A):\n",
    "    #sanity check\n",
    "#     print(\"back_prop\")\n",
    "#     print(\"X shape:\" + str(X.shape))\n",
    "#     print(\"Y shape:\" + str(Y.shape))\n",
    "#     print(\"A shape:\" + str(A.shape))\n",
    "    dW = np.dot(X, (A-Y))/X.shape[1]\n",
    "    db = np.sum(A - Y)/X.shape[1]\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated our gradients, we can use gradient descent algorithm to update our parameters.\n",
    "Gradient Descent : https://en.wikipedia.org/wiki/Gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent\n",
    "def gradient_descent(W, b, dW, db, learning_rate = 0.01):\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined all of our helper functions, we can puttogether our logistic regresion model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression function !\n",
    "def logistic_regression(X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    m = X_train.shape[1] #number of training examples\n",
    "    W, b = initialize_parameters(X_train.shape[0]) #initialize learning parameters\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        A = forward_prop(X, W, b)\n",
    "        cost = compute_cost(Y, A)\n",
    "        dW, db = back_prop(X, Y, A)\n",
    "        W, b = gradient_descent(W, b, dW, db, learning_rate)\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "                \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our logistic regression model , we need to make predictions for our validation dataset and check their accuracy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions !\n",
    "def predict(X_val, W, b):\n",
    "    predictions = forward_prop(X_val, W, b)\n",
    "    \n",
    "    #map predictions below 0.5 to 0 and above 0.5 to 1\n",
    "    predictions[predictions > 0.5] = int(1)\n",
    "    predictions[predictions < 0.5] = int(0)\n",
    "    return predictions\n",
    "\n",
    "# calculate accuracy\n",
    "def test_accuracy(predictions, Y_val):\n",
    "    accuracy = np.sum(predictions == Y_val)/predictions.shape[0]*100\n",
    "    print(\"Accuracy on validation data :\" +str(accuracy)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally , lets Run our code !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.601478\n",
      "Cost after iteration 200: 0.551665\n",
      "Cost after iteration 300: 0.522173\n",
      "Cost after iteration 400: 0.503476\n",
      "Cost after iteration 500: 0.490978\n",
      "Cost after iteration 600: 0.482266\n",
      "Cost after iteration 700: 0.475985\n",
      "Cost after iteration 800: 0.471326\n",
      "Cost after iteration 900: 0.467787\n",
      "Cost after iteration 1000: 0.465043\n",
      "Cost after iteration 1100: 0.462876\n",
      "Cost after iteration 1200: 0.461139\n",
      "Cost after iteration 1300: 0.459725\n",
      "Cost after iteration 1400: 0.458561\n",
      "Cost after iteration 1500: 0.457592\n",
      "Cost after iteration 1600: 0.456777\n",
      "Cost after iteration 1700: 0.456085\n",
      "Cost after iteration 1800: 0.455494\n",
      "Cost after iteration 1900: 0.454984\n",
      "Cost after iteration 2000: 0.454543\n",
      "Cost after iteration 2100: 0.454157\n",
      "Cost after iteration 2200: 0.453820\n",
      "Cost after iteration 2300: 0.453523\n",
      "Cost after iteration 2400: 0.453260\n",
      "Cost after iteration 2500: 0.453027\n",
      "Cost after iteration 2600: 0.452820\n",
      "Cost after iteration 2700: 0.452635\n",
      "Cost after iteration 2800: 0.452469\n",
      "Cost after iteration 2900: 0.452321\n",
      "Cost after iteration 3000: 0.452188\n",
      "Cost after iteration 3100: 0.452067\n",
      "Cost after iteration 3200: 0.451959\n",
      "Cost after iteration 3300: 0.451861\n",
      "Cost after iteration 3400: 0.451773\n",
      "Cost after iteration 3500: 0.451693\n",
      "Cost after iteration 3600: 0.451620\n",
      "Cost after iteration 3700: 0.451555\n",
      "Cost after iteration 3800: 0.451495\n",
      "Cost after iteration 3900: 0.451441\n",
      "Cost after iteration 4000: 0.451392\n",
      "Cost after iteration 4100: 0.451347\n",
      "Cost after iteration 4200: 0.451306\n",
      "Cost after iteration 4300: 0.451269\n",
      "Cost after iteration 4400: 0.451235\n",
      "Cost after iteration 4500: 0.451205\n",
      "Cost after iteration 4600: 0.451177\n",
      "Cost after iteration 4700: 0.451151\n",
      "Cost after iteration 4800: 0.451128\n",
      "Cost after iteration 4900: 0.451107\n",
      "Cost after iteration 5000: 0.451087\n",
      "Cost after iteration 5100: 0.451070\n",
      "Cost after iteration 5200: 0.451054\n",
      "Cost after iteration 5300: 0.451039\n",
      "Cost after iteration 5400: 0.451026\n",
      "Cost after iteration 5500: 0.451014\n",
      "Cost after iteration 5600: 0.451003\n",
      "Cost after iteration 5700: 0.450992\n",
      "Cost after iteration 5800: 0.450983\n",
      "Cost after iteration 5900: 0.450975\n",
      "Cost after iteration 6000: 0.450967\n",
      "Cost after iteration 6100: 0.450960\n",
      "Cost after iteration 6200: 0.450954\n",
      "Cost after iteration 6300: 0.450948\n",
      "Cost after iteration 6400: 0.450942\n",
      "Cost after iteration 6500: 0.450937\n",
      "Cost after iteration 6600: 0.450933\n",
      "Cost after iteration 6700: 0.450929\n",
      "Cost after iteration 6800: 0.450925\n",
      "Cost after iteration 6900: 0.450922\n",
      "Cost after iteration 7000: 0.450919\n",
      "Cost after iteration 7100: 0.450916\n",
      "Cost after iteration 7200: 0.450913\n",
      "Cost after iteration 7300: 0.450911\n",
      "Cost after iteration 7400: 0.450909\n",
      "Cost after iteration 7500: 0.450907\n",
      "Cost after iteration 7600: 0.450905\n",
      "Cost after iteration 7700: 0.450904\n",
      "Cost after iteration 7800: 0.450902\n",
      "Cost after iteration 7900: 0.450901\n",
      "Cost after iteration 8000: 0.450900\n",
      "Cost after iteration 8100: 0.450898\n",
      "Cost after iteration 8200: 0.450897\n",
      "Cost after iteration 8300: 0.450896\n",
      "Cost after iteration 8400: 0.450896\n",
      "Cost after iteration 8500: 0.450895\n",
      "Cost after iteration 8600: 0.450894\n",
      "Cost after iteration 8700: 0.450893\n",
      "Cost after iteration 8800: 0.450893\n",
      "Cost after iteration 8900: 0.450892\n",
      "Cost after iteration 9000: 0.450892\n",
      "Cost after iteration 9100: 0.450891\n",
      "Cost after iteration 9200: 0.450891\n",
      "Cost after iteration 9300: 0.450891\n",
      "Cost after iteration 9400: 0.450890\n",
      "Cost after iteration 9500: 0.450890\n",
      "Cost after iteration 9600: 0.450890\n",
      "Cost after iteration 9700: 0.450889\n",
      "Cost after iteration 9800: 0.450889\n",
      "Cost after iteration 9900: 0.450889\n"
     ]
    }
   ],
   "source": [
    "costs = [] #store cost to plot against iterations\n",
    "W, b = logistic_regression(X_train, Y_train, 10000, 0.01, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the accuracy of our predictions using these parameters !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation data :79.47761194029852%\n"
     ]
    }
   ],
   "source": [
    "#make predictions on our validation dataset\n",
    "preds = predict(X_val, W, b)\n",
    "#calculate accuracy of our predictions\n",
    "test_accuracy(preds, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , we use our trained parameters to make predictions on our test dataset and save the results as CSV ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import test dataset\n",
    "X_test = data_test_cleaned.values\n",
    "\n",
    "#standardize test dataset\n",
    "X_test = standardize(X_test)\n",
    "X_test = X_test.T\n",
    "\n",
    "#make predictions\n",
    "predictions_test = predict(X_test, W, b).astype(int)\n",
    "\n",
    "#compile into a dataframe\n",
    "predictions_df = pd.DataFrame({ 'PassengerId': data_test[\"PassengerId\"], 'Survived': predictions_test[:,0]})\n",
    "\n",
    "#export dataframe as csv\n",
    "predictions_df.to_csv(\"Logistic_regression.csv\", index = False)\n",
    "\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets about 65% accuracy on test dataset according to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
